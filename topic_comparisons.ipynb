{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, os, torch, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from cuml.preprocessing import Normalizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- 5/8/24\n",
    "    - Created file\n",
    "    - added *topics_over_time* function:\n",
    "        - function that takes document dataframe (with topic assignments and bin_period) and label reperesentations and transforms the data into a matrix representing the relative min and maximum frequencies for each topics\n",
    "        - additionally displays information on a heatmap\n",
    "    - added *gpu_cosine_similarity* function:\n",
    "        - uses cuml's normalizer to speed up cosine similarity calculation\n",
    "    - **Will need to address label representations eventually**\n",
    "    - added **topic comparisions**\n",
    "        - takes topic embeddings and calculates cosine similarity between them \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes docs and transforms into a relative topic frequency matrix based on bin period\n",
    "def topics_over_time(documents, topic_labels, save_img=None, title=None, index=None):\n",
    "    topic_frequency_norm = documents.groupby('Topic')['bin_period'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    #in case bins were not split correctly\n",
    "    if len(topic_frequency_norm.columns) == 21:\n",
    "        df = topic_frequency_norm.drop(20, axis=1)\n",
    "    else:\n",
    "        df = topic_frequency_norm\n",
    "    #min max normalization\n",
    "    df_norm = df.sub(df.min(axis=1), axis=0).div((df.max(axis=1) - df.min(axis=1)), axis=0)\n",
    "    df_norm = df_norm.reset_index()\n",
    "    if index is not None:\n",
    "        df_norm = np.take(df_norm, index, axis=0)\n",
    "    #creates extra column specifing the bin period where min-max score = 1, for easier organization in heatmap\n",
    "    df_norm['largest'] = df_norm.iloc[:,1:21].values.argmax(axis=1)\n",
    "   \n",
    "    df_norm = df_norm.sort_values('largest')\n",
    "    df_norm = df_norm.reset_index()\n",
    "    sorted_index = [topic_labels[i] for i in df_norm['index']]\n",
    "    \n",
    "    #plotting matrix\n",
    "    fig = px.imshow(df_norm.iloc[:,2:22], width=1400, height=2200, aspect=\"auto\", color_continuous_scale='deep')\n",
    "    fig.update_layout(\n",
    "       yaxis=dict(\n",
    "            tickvals=list(range(len(df_norm['index']))),\n",
    "           ticktext=sorted_index,\n",
    "           tickfont=dict(size=13),\n",
    "           title = dict(font_size=18,text=\"<i><b>Topic Labels</b></i>\")),\n",
    "           xaxis=dict(title = dict(font_size=18,text=\"<i><b>Bin Period</b></i>\")),\n",
    "    margin=dict(l=600)\n",
    "    )\n",
    "    if title is not None:\n",
    "        fig.update_layout(\n",
    "        title=dict(text=title, y=0.97, x=0.5, xanchor='center')\n",
    "            )\n",
    "    fig.show()\n",
    "    if save_img is not None:\n",
    "        fig.write_image(save_img)\n",
    "\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replication of scikit-learn's cosine similarity, with cuml's normalizer for speed\n",
    "def gpu_cosine_similarity(matrix, matrix2=None, max=None):\n",
    "    norm = Normalizer()\n",
    "    norm_matrix = norm.transform(matrix)\n",
    "    if max is None:\n",
    "        if matrix2 is None:\n",
    "            K = safe_sparse_dot(norm_matrix, norm_matrix.T, dense_output=True) \n",
    "        else:\n",
    "            norm_matrix2 = norm.transform(matrix2)\n",
    "            K = safe_sparse_dot(norm_matrix, norm_matrix2.T)\n",
    "        return K\n",
    "    else\n",
    "        values = []\n",
    "        if matrix2 is None:\n",
    "            num_samples = round(math.sqrt(number))\n",
    "            for n in range(max):\n",
    "                i = np.random\n",
    "                vectorA = \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_docs(documents, data, topic_model, save_dir=None):\n",
    "    docs = documents\n",
    "    docs['Year'] = pd.to_datetime(data['Date']).dt.year\n",
    "    docs['Journal'] = data['Journal']\n",
    "   # docs = docs.drop(columns='ID')\n",
    "\n",
    "    docs['Date'] = pd.to_datetime(data['Date'])\n",
    "# documents['Timestamp'] = documents['Date'].dt.timestamp()\n",
    "    ts_for_bins = list(docs['Date'])\n",
    "    ts_for_bins.sort()\n",
    "\n",
    "    bin_num  = 20\n",
    "    bin_size = int(len(ts_for_bins)/bin_num)\n",
    "    bin_idxs = [idx for idx in range(0, len(ts_for_bins), bin_size)]\n",
    "\n",
    "    bin_timestamps = [ts_for_bins[idx] for idx in bin_idxs]\n",
    "\n",
    "    max_timestamp      = max(ts_for_bins) + pd.Timedelta(1, unit='D')\n",
    "\n",
    "    bin_df         = pd.DataFrame(list(zip(bin_idxs, bin_timestamps)),\n",
    "            columns=['bin_start_idx', 'bin_start_date'])\n",
    "\n",
    "    bin_df['Count'] = bin_df['bin_start_idx'].diff().fillna(bin_df['bin_start_idx'].iloc[0]).astype(int)\n",
    "    bin_df['bin_end_date'] = bin_df['bin_start_date'].shift(-1) - pd.Timedelta(days=1)\n",
    "    bin_df['bin_end_date'][20] = max(docs['Date']) + pd.Timedelta(1, unit='D')\n",
    "\n",
    "    bin_period = []\n",
    "    docs['bin_period'] = 0\n",
    "\n",
    "    for i in tqdm(range(len(docs))):\n",
    "        period = 0\n",
    "        while docs['Date'][i] > bin_df['bin_end_date'][period] and period < len(bin_df):\n",
    "            period +=1\n",
    "        \n",
    "            # doc = documents['Date'][i]\n",
    "            # bindate = bin_df['bin_end_date'][period]\n",
    "\n",
    "            # print(f'Period: {period}.. {doc} < {bindate}')\n",
    "\n",
    "    # print(f'Assigning Document: {i} bin: {period}')\n",
    "        docs['bin_period'][i] = period\n",
    "\n",
    "    if save_dir is not None:\n",
    "        with open(save_dir/'new_docs.pickle', \"wb\") as f:\n",
    "            pickle.dump(docs, f)\n",
    "        print(f\"Docs saved at: {save_dir}\")\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in data from journals 0-264\n",
    "\n",
    "dir_0full = Path('/mnt/scratch/ande2472/model_output/topic_modeling/0_to_264/')\n",
    "\n",
    "topic_model_0 = BERTopic.load(dir_0full/'model_outliers_reduced')\n",
    "# with open(dir_0full/'new_docs.pickle', \"rb\") as f:\n",
    "#   docs_0 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_0.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(topic_model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two options for label generation, will need to be addressed later\n",
    "\n",
    "topic_labels2 = topic_model_0.generate_topic_labels(nr_words=4, word_length=15, aspect=\"MMR\", separator='|')\n",
    "topic_labels = topic_model_0.generate_topic_labels(nr_words=3, word_length=20, aspect=\"KeyBERT\", separator='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = topics_over_time(docs_0, topic_labels, title=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic comparisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates topic by topic similarity matrix\n",
    "topic_embeddings = topic_model_0.topic_embeddings_\n",
    "sim_matrix = gpu_cosine_similarity(topic_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing similarity of first 20 topics to eachother\n",
    "fig = ff.create_distplot(sim_matrix[:19], topic_labels2[:19],bin_size=.025)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### within topic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document within topic similarity matrix calculated in topicsimilarity.py\n",
    "# example only for topic 1 of journals 0-264\n",
    "\n",
    "\n",
    "dir_0full = Path('/mnt/scratch/ande2472/sjrouts/0to264_full/')\n",
    "save_dir = dir_0full/'sim_matrix_test.pickle'\n",
    "with open(save_dir, \"rb\") as f:\n",
    "    sim_mat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_0full = Path('/mnt/scratch/ande2472/sjrouts/0to264_full/')\n",
    "with open(dir_0full/'new_docs.pickle', \"rb\") as f:\n",
    "        docs_0 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = list(docs_0[docs_0['Topic'] == 1].index)\n",
    "topic_unlist = list(docs_0[docs_0['Topic'] != 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a topic with 55033, there are 3 billion (3028631089) entries.... idk if scaling up will be possible for between topic similarity\n",
    "upper_triangle_no_diag = sim_mat[np.triu_indices_from(sim_mat, k=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = np.max(upper_triangle_no_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = np.min(upper_triangle_no_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.arange(0, 1, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count['Count'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in tqdm(upper_triangle_no_diag):\n",
    "        bin = 0\n",
    "        while num > bin_count['Count'][bin] and bin > 38:\n",
    "            bin_count +=1\n",
    "        bin_count['Count'][bin] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_indices = np.digitize(upper_triangle_no_diag, array, right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_counts = np.bincount(bin_indices, minlength=len(array))\n",
    "bin_counts[-2] += bin_counts[-1]\n",
    "bin_counts = bin_counts[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count_df = pd.DataFrame({'Bin': array, 'Count': bin_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=bin_count_df['Bin'],\n",
    "        y=bin_count_df['Count'],\n",
    "        width=0.02  # Adjust the width to match the bin spacing\n",
    "    )\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Histogram of Bin Values and Counts',\n",
    "    xaxis_title='Bin Value',\n",
    "    yaxis_title='Count',\n",
    "    bargap=0.2  # Adjust gap between bars\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_distplot(sim_matrix[:19], topic_labels2[:19],bin_size=.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_histogram(array):\n",
    "    max_val = np.max(array)\n",
    "    min_val = np.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#too slow\n",
    "fig = ff.create_distplot([upper_triangle_no_diag],['Topic 1'] ,bin_size=.025)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=bin_count_df['Bin'],\n",
    "        y=bin_count_df['Count'],\n",
    "        width=0.02  # Adjust the width to match the bin spacing\n",
    "    )\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Histogram of Bin Values and Counts',\n",
    "    xaxis_title='Bin Value',\n",
    "    yaxis_title='Count',\n",
    "    bargap=0.2  # Adjust gap between bars\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([[3,4,5,6],[2,3,2,3],[3,4,5,3]])\n",
    "array2 = np.array([[3,4,5,6],[2,3,2,3],[2,3,2,3],[3,4,5,3],[3,4,5,3],[3,4,5,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replication of scikit-learn's cosine similarity, with cuml's normalizer for speed\n",
    "def gpu_cosine_similarity(matrix, matrix2=None, max=None):\n",
    "    norm = Normalizer()\n",
    "    norm_matrix = norm.transform(matrix)\n",
    "    if max is None:\n",
    "        if matrix2 is None:\n",
    "            K = safe_sparse_dot(norm_matrix, norm_matrix.T, dense_output=True) \n",
    "        else:\n",
    "            norm_matrix2 = norm.transform(matrix2)\n",
    "            K = safe_sparse_dot(norm_matrix, norm_matrix2.T)\n",
    "        return K\n",
    "    else:\n",
    "        similarity_values = []\n",
    "        if matrix2 is None:\n",
    "            num_samples = round(math.sqrt(max))\n",
    "            idx1 = np.random.randint(0, matrix.shape[0], num_samples)\n",
    "            idx2 = np.random.randint(0, matrix.shape[0], num_samples)\n",
    "            for i in idx1:\n",
    "                for j in idx2:\n",
    "                    similarity_values += [np.dot(norm_matrix[i],norm_matrix[j])]\n",
    "        else:\n",
    "            norm_matrix2 = norm.transform(matrix2)\n",
    "            num_samples = round(math.sqrt(max))\n",
    "            idx1 = np.random.randint(0, matrix.shape[0], num_samples)\n",
    "            idx2 = np.random.randint(0, matrix.shape[0], num_samples)\n",
    "            for i in idx1:\n",
    "                for j in idx2:\n",
    "                    similarity_values += [np.dot(norm_matrix[i],norm_matrix2[j])]\n",
    "        return similarity_values\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpu_cosine_similarity(array1,array2, max=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [2,4,5]\n",
    "array[-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = Path('/mnt/scratch/ande2472/data/0_topjournals/')\n",
    "with open(file_dir/'0_topjournals_embs.pickle', \"rb\") as f:\n",
    "    emb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv(file_dir/'0_topjournals.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = docs.drop(['Unnamed: 0','index','Title','Abstract'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents['Topic'] = topic_model_0.topics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_0full/'new_docs.pickle', \"wb\") as f:\n",
    "        pickle.dump(documents,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_0 = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_similarity = gpu_cosine_similarity(emb[topic_list],max=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_similarity = gpu_cosine_similarity(emb[topic_unlist],max=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_distplot([within_similarity,between_similarity], ['Within','Between'],bin_size=.025, show_rug=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = list(docs_0[docs_0['Topic'] == 38].index)\n",
    "topic_unlist = list(docs_0[docs_0['Topic'] != 38].index)\n",
    "within_similarity = gpu_cosine_similarity(emb[topic_list],max=1000000)\n",
    "between_similarity = gpu_cosine_similarity(emb[topic_unlist],max=1000000)\n",
    "fig = ff.create_distplot([within_similarity,between_similarity], ['Within','Between'],bin_size=.025, show_rug=False)\n",
    "fig.update_layout(title_text='Topic 38')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = list(docs_0[docs_0['Topic'] == 78].index)\n",
    "topic_unlist = list(docs_0[docs_0['Topic'] != 78].index)\n",
    "within_similarity = gpu_cosine_similarity(emb[topic_list],max=1000000)\n",
    "between_similarity = gpu_cosine_similarity(emb[topic_unlist],max=1000000)\n",
    "colors = ['rgb(0, 200, 200)','rgb(0, 0, 100)']\n",
    "fig = ff.create_distplot([within_similarity,between_similarity], ['Within Topic','Between Topic'],bin_size=.025, show_rug=False, colors=colors)\n",
    "\n",
    "fig.update_layout(title_text='Document Similarity Distribution for Topic 78')\n",
    "fig.update_xaxes(title_text='Cosine Similarity Score')\n",
    "fig.update_yaxes(showgrid=False, title_text='Relative Frequency')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_within_between(emb, docs, labels, save_dir, max_val=1000000, topics=None ):\n",
    "    if topics is None:\n",
    "        for topic in tqdm(docs['Topic'].unique()):\n",
    "            topic_list = list(docs[docs['Topic'] == topic].index)\n",
    "            topic_unlist = list(docs[docs['Topic'] != topic].index)\n",
    "            within_similarity = gpu_cosine_similarity(emb[topic_list],max=max_val)\n",
    "            between_similarity = gpu_cosine_similarity(emb[topic_unlist],max=max_val)\n",
    "            colors = ['rgb(0, 200, 200)','rgb(0, 0, 100)']\n",
    "            fig = ff.create_distplot([within_similarity,between_similarity], ['Within Topic','Between Topic'],bin_size=.025, show_rug=False, colors=colors)\n",
    "            label = labels[topic]\n",
    "            fig.update_layout(title_text=f'Document Similarity Distribution for {label}')\n",
    "            \n",
    "            fig.update_xaxes(title_text='Cosine Similarity Score')\n",
    "            fig.update_yaxes(showgrid=False, title_text='Relative Frequency')\n",
    "            \n",
    "            fig.write_image(save_dir/f\"topic_{topic}_within_between_sim.pdf\")\n",
    "         \n",
    "\n",
    "    else:\n",
    "        for topic in tqdm(topics):\n",
    "            topic_list = list(docs[docs['Topic'] == topic].index)\n",
    "            topic_unlist = list(docs[docs['Topic'] != topic].index)\n",
    "            within_similarity = gpu_cosine_similarity(emb[topic_list],max=max_val)\n",
    "            between_similarity = gpu_cosine_similarity(emb[topic_unlist],max=max_val)\n",
    "            colors = ['rgb(0, 200, 200)','rgb(0, 0, 100)']\n",
    "            fig = ff.create_distplot([within_similarity,between_similarity], ['Within Topic','Between Topic'],bin_size=.025, show_rug=False, colors=colors)\n",
    "            label = labels[topic]\n",
    "            fig.update_layout(title_text=f'Document Similarity Distribution for {label}')\n",
    "            \n",
    "            fig.update_xaxes(title_text='Cosine Similarity Score')\n",
    "            fig.update_yaxes(showgrid=False, title_text='Relative Frequency')\n",
    "   \n",
    "            fig.write_image(save_dir/f\"topic_{topic}_within_between_sim.pdf\")\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_dir = Path('/mnt/scratch/ande2472/model_output/topic_modeling/0_to_264/plots')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_within_between(emb, docs_0, topic_labels, save_dir=plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdbscan5",
   "language": "python",
   "name": "hdbscan5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1283819c91109657474ee241b0e2d53f5c4cedff6ed7ba5859c17719aa31188"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
